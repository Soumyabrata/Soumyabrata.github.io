---
layout: post
title: "ECCV 2016: Deep learning, pit stop or plateau?"
description: "Do we have a pit stop or a plateau?"
comments: true
---

This is my customary update of percentage of <i>deep-learning papers</i> in the major vision conferences. At CVPR 2015, I described its [exponential growth]({{ site.url }}/deep-learning-scraping/), after ICCV 2015 I wondered whether it had [plateaued]({{ site.url }}/deep-learning-plateau/), and in CVPR 2016 it [took over again]({{ site.url }}/deep-learning-plateau/deep-learning-takes-over-again/).<br>
<br>
Here the evolution of the results as of ECCV 2016:
{% highlight python%}
CVPR2013:  0.85% (  4 out of 471)
ICCV2013:  1.54% (  7 out of 455)
CVPR2014:  3.70% ( 20 out of 540)
CVPR2015: 14.45% ( 87 out of 602)
ICCV2015: 14.45% ( 76 out of 526)
CVPR2016: 23.48% (151 out of 643)
ECCV2016: 20.96% ( 87 out of 415)
{% endhighlight %}

And the plot using XKCD style, as described in this [blog post]({{ site.url }}/xkcd-deep-learning/):
<br />
<br />
<img align="middle" width="500" src="{{ site.url }}/images/xkcd_deep2.png" alt="...">
<br />
<br />

So in ICCV15 we had in fact a pit stop which has been surpassed by CVPR16 and ECCV16.
Will ECCV16 be also a pit stop or we will be finally saturating?<br><br>

It is also interesting to note the smaller size of ECCV compared to the rest. I prefer it this way, to be honest:
<blockquote class="twitter-tweet" data-lang="ca"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/CVPR2016?src=hash">#CVPR2016</a> reflexions<br>- Orals with papers at arXiv long time before and even outdated already.<br>- Posters too crowded to discuss with authors.</p>&mdash; Jordi Pont-Tuset (@jponttuset) <a href="https://twitter.com/jponttuset/status/748199856930357250">29 de juny de 2016</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
See you in Amsterdam!